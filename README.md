# Tools-Expermineting-AI-LLMS

1. Token-A token is a unit of text—such as a word, subword, or character—that the model processes individually.
   1 Token = 0.75 words
   so, 1000 tokens is 750 words!!

#### To Understand the tokenization in LLMs- (https://platform.openai.com/tokenizer)
