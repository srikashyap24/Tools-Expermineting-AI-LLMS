# Tools-Expermineting-AI-LLMS

1. Token-A token is a unit of text—such as a word, subword, or character—that the model processes individually.
   1 Token = 0.75 words
   so, 1000 tokens is 750 words!!

#### To Understand the tokenization in LLMs- (https://platform.openai.com/tokenizer)

2. Context Window- Context window is the maximum amount of text (tokens) the model can consider at once when processing input and generating output.

#### To undertand leaderboard of LLMs and their cost with respect to context Window size and Tokens- (https://www.vellum.ai/llm-leaderboard)

3. A Transformer is a neural network architecture that uses self-attention mechanisms(is a mechanism that allows a model to weigh and relate different parts of the same input sequence to understand context and dependencies between words, regardless of their positions) to process and generate sequences of data efficiently in parallel.
