# Tools-Expermineting-AI-LLMS

1. Token-A token is a unit of text—such as a word, subword, or character—that the model processes individually.
   1 Token = 0.75 words
   so, 1000 tokens is 750 words!!

#### To Understand the tokenization in LLMs- (https://platform.openai.com/tokenizer)

2. Context Window- Context window is the maximum amount of text (tokens) the model can consider at once when processing input and generating output.

#### To undertand leaderboard of LLMs and their cost with respect to context Window size and Tokens- (https://www.vellum.ai/llm-leaderboard)
